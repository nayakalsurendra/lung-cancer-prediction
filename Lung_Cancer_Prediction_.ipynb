{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0e3c2722",
      "metadata": {
        "id": "0e3c2722"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib. pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2377dd1f",
      "metadata": {
        "id": "2377dd1f",
        "outputId": "c50f9b01-d0e4-4061-faec-3198ecacae8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'survey lung cancer.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f8ebf1e7baa1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlung_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"survey lung cancer.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'survey lung cancer.csv'"
          ]
        }
      ],
      "source": [
        "lung_data = pd.read_csv(\"survey lung cancer.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a5d227",
      "metadata": {
        "id": "f3a5d227"
      },
      "outputs": [],
      "source": [
        "lung_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5865b5f3",
      "metadata": {
        "id": "5865b5f3"
      },
      "outputs": [],
      "source": [
        "lung_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f77a74d",
      "metadata": {
        "id": "1f77a74d"
      },
      "outputs": [],
      "source": [
        "lung_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27526252",
      "metadata": {
        "id": "27526252"
      },
      "outputs": [],
      "source": [
        "#dependent_variable\n",
        "x = lung_data.iloc[:,0:-1]\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53880d7",
      "metadata": {
        "id": "c53880d7"
      },
      "outputs": [],
      "source": [
        "#independent_variable\n",
        "y = lung_data. iloc[:,-1:]\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e96d026",
      "metadata": {
        "id": "0e96d026"
      },
      "outputs": [],
      "source": [
        "lung_data.GENDER = lung_data.GENDER.map({\"M\":1,\"F\":2})\n",
        "lung_data.LUNG_CANCER = lung_data.LUNG_CANCER.map({\"YES\":1,\"NO\":2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9ea8a4",
      "metadata": {
        "id": "9b9ea8a4"
      },
      "outputs": [],
      "source": [
        "lung_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a5e5bc",
      "metadata": {
        "id": "51a5e5bc"
      },
      "outputs": [],
      "source": [
        "lung_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b56be0",
      "metadata": {
        "id": "b3b56be0"
      },
      "outputs": [],
      "source": [
        "lung_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f38592",
      "metadata": {
        "id": "44f38592"
      },
      "outputs": [],
      "source": [
        "lung_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2592d4e3",
      "metadata": {
        "id": "2592d4e3"
      },
      "outputs": [],
      "source": [
        "lung_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e7cdf1",
      "metadata": {
        "id": "e9e7cdf1"
      },
      "outputs": [],
      "source": [
        "#the describe() method returns description of data in DataFrame\n",
        "lung_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18426938",
      "metadata": {
        "id": "18426938"
      },
      "outputs": [],
      "source": [
        "#the info() method prints information of the database\n",
        "lung_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a37db6e",
      "metadata": {
        "id": "7a37db6e"
      },
      "outputs": [],
      "source": [
        "#Splitting the Dataset: Training and Testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=1/3,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfead0c0",
      "metadata": {
        "scrolled": true,
        "id": "bfead0c0"
      },
      "outputs": [],
      "source": [
        "lung_data['LUNG_CANCER'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9286c3f",
      "metadata": {
        "id": "d9286c3f"
      },
      "outputs": [],
      "source": [
        "len(lung_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa00fa40",
      "metadata": {
        "id": "fa00fa40"
      },
      "outputs": [],
      "source": [
        "len(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37951afd",
      "metadata": {
        "id": "37951afd"
      },
      "outputs": [],
      "source": [
        "len(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfb72b5",
      "metadata": {
        "id": "4cfb72b5"
      },
      "outputs": [],
      "source": [
        "#dependent_variable\n",
        "x = lung_data.iloc[:,0:-1]\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f6d150",
      "metadata": {
        "id": "d7f6d150"
      },
      "outputs": [],
      "source": [
        "#independent_variable\n",
        "y = lung_data.iloc[:,-1:]\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4e3ac8",
      "metadata": {
        "id": "4d4e3ac8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fd1145",
      "metadata": {
        "id": "c5fd1145"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=1/3,random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ad4cc5",
      "metadata": {
        "id": "d8ad4cc5"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97e35e5",
      "metadata": {
        "id": "f97e35e5"
      },
      "outputs": [],
      "source": [
        "#Fitting simple linear regression to the training test\n",
        "Model1 = LogisticRegression()\n",
        "Model1.fit(x_train, y_train)\n",
        "#Predicting the test set results\n",
        "prediction1 = Model1.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5172ffc",
      "metadata": {
        "id": "d5172ffc"
      },
      "outputs": [],
      "source": [
        "prediction1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6001a0ef",
      "metadata": {
        "id": "6001a0ef"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "confusion_matrix(y_test,prediction1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a045421",
      "metadata": {
        "id": "0a045421"
      },
      "outputs": [],
      "source": [
        " accuracy_score(y_test,prediction1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7690c64",
      "metadata": {
        "id": "c7690c64"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "probs = Model1.predict_proba(x_test)\n",
        "precision_score(y_test, prediction1, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6310670c",
      "metadata": {
        "id": "6310670c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively\n",
        "accuracy = accuracy_score(y_test, prediction1)\n",
        "precision = precision_score(y_test, prediction1)\n",
        "recall = recall_score(y_test, prediction1)\n",
        "f1 = f1_score(y_test, prediction1)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f209fb5",
      "metadata": {
        "id": "9f209fb5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "061a23a7",
      "metadata": {
        "id": "061a23a7"
      },
      "outputs": [],
      "source": [
        "recall_score(y_test, prediction1, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bff9f73",
      "metadata": {
        "id": "7bff9f73"
      },
      "outputs": [],
      "source": [
        " f1_score(y_test, prediction1, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db6e38f",
      "metadata": {
        "id": "9db6e38f"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true = y_test, y_pred = prediction1)\n",
        "#plot_confusion_matrix(cm,level,title = \"confusion_matrix\")\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a93084",
      "metadata": {
        "id": "b1a93084"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a644d5cb",
      "metadata": {
        "id": "a644d5cb"
      },
      "outputs": [],
      "source": [
        " from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaad0759",
      "metadata": {
        "id": "aaad0759"
      },
      "outputs": [],
      "source": [
        "#Fitting K-NN to the Training set\n",
        "classifier = KNeighborsClassifier(n_neighbors = 3, metric = \"minkowski\", p = 2)\n",
        "classifier.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6747f83",
      "metadata": {
        "id": "b6747f83"
      },
      "outputs": [],
      "source": [
        "#Predicting the Test set result\n",
        "prediction2 = classifier.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b29cd9e",
      "metadata": {
        "id": "5b29cd9e"
      },
      "outputs": [],
      "source": [
        "prediction2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0338e9e",
      "metadata": {
        "id": "e0338e9e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "confusion_matrix(y_test,prediction2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240a8f74",
      "metadata": {
        "id": "240a8f74"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively\n",
        "accuracy = accuracy_score(y_test, prediction2)\n",
        "precision = precision_score(y_test, prediction2)\n",
        "recall = recall_score(y_test, prediction2)\n",
        "f1 = f1_score(y_test, prediction2)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52397e3f",
      "metadata": {
        "id": "52397e3f"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,prediction2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250b49e8",
      "metadata": {
        "id": "250b49e8"
      },
      "outputs": [],
      "source": [
        "probs = Model1.predict_proba(x_test)\n",
        "precision_score(y_test, prediction2, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51acb9ae",
      "metadata": {
        "id": "51acb9ae"
      },
      "outputs": [],
      "source": [
        "recall_score(y_test, prediction2, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e05028",
      "metadata": {
        "id": "c1e05028"
      },
      "outputs": [],
      "source": [
        "f1_score(y_test, prediction2, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcce3018",
      "metadata": {
        "id": "dcce3018"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true = y_test, y_pred = prediction2)\n",
        "#plot_confusion_matrix(cm,level,title = \"confusion_matrix\")\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6627e1a",
      "metadata": {
        "id": "b6627e1a"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee2f7bb",
      "metadata": {
        "id": "4ee2f7bb"
      },
      "outputs": [],
      "source": [
        "#Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier(random_state = 0,criterion = \"entropy\")\n",
        "tree.fit(x_train, y_train)\n",
        "prediction3 = classifier.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d144687",
      "metadata": {
        "id": "4d144687"
      },
      "outputs": [],
      "source": [
        "prediction3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb70491",
      "metadata": {
        "id": "1eb70491"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test,prediction3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113e06b6",
      "metadata": {
        "id": "113e06b6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively\n",
        "accuracy = accuracy_score(y_test, prediction3)\n",
        "precision = precision_score(y_test, prediction3)\n",
        "recall = recall_score(y_test, prediction3)\n",
        "f1 = f1_score(y_test, prediction3)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee956ae",
      "metadata": {
        "id": "1ee956ae"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,prediction3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0481b6",
      "metadata": {
        "id": "1c0481b6"
      },
      "outputs": [],
      "source": [
        "probs = Model1.predict_proba(x_test)\n",
        "precision_score(y_test, prediction3, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4ad4eb",
      "metadata": {
        "id": "1b4ad4eb"
      },
      "outputs": [],
      "source": [
        "recall_score(y_test, prediction3, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef6eb27",
      "metadata": {
        "id": "0ef6eb27"
      },
      "outputs": [],
      "source": [
        "f1_score(y_test, prediction3, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55503779",
      "metadata": {
        "id": "55503779"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true = y_test, y_pred = prediction3)\n",
        "#plot_confusion_matrix(cm,level,title = \"confusion_matrix\")\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e52430",
      "metadata": {
        "id": "c8e52430"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5494adba",
      "metadata": {
        "id": "5494adba"
      },
      "outputs": [],
      "source": [
        "#Support Vector Machine\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "svm = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9,probability=True),n_jobs=-1))\n",
        "svm.fit(x_train, y_train)\n",
        "prediction4 = svm.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca742075",
      "metadata": {
        "scrolled": true,
        "id": "ca742075"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test,prediction4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788c9771",
      "metadata": {
        "id": "788c9771"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively\n",
        "accuracy = accuracy_score(y_test, prediction4)\n",
        "precision = precision_score(y_test, prediction4)\n",
        "recall = recall_score(y_test, prediction4)\n",
        "f1 = f1_score(y_test, prediction4)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56fb954d",
      "metadata": {
        "id": "56fb954d"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,prediction4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553761fd",
      "metadata": {
        "id": "553761fd"
      },
      "outputs": [],
      "source": [
        "probs = Model1.predict_proba(x_test)\n",
        "precision_score(y_test, prediction4, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6baec25b",
      "metadata": {
        "id": "6baec25b"
      },
      "outputs": [],
      "source": [
        "recall_score(y_test, prediction4, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab6a107",
      "metadata": {
        "id": "aab6a107"
      },
      "outputs": [],
      "source": [
        "f1_score(y_test, prediction4, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7698178",
      "metadata": {
        "id": "a7698178"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true = y_test, y_pred = prediction4)\n",
        "#plot_confusion_matrix(cm,level,title = \"confusion_matrix\")\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1656a9",
      "metadata": {
        "id": "bb1656a9"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3063bc8c",
      "metadata": {
        "id": "3063bc8c"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "nbcla = GaussianNB()\n",
        "nbcla.fit(x_train, y_train)\n",
        "prediction5 = nbcla.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e2e56b",
      "metadata": {
        "id": "02e2e56b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "confusion_matrix(y_test,prediction5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7379ab57",
      "metadata": {
        "id": "7379ab57"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively\n",
        "accuracy = accuracy_score(y_test, prediction5)\n",
        "precision = precision_score(y_test, prediction5)\n",
        "recall = recall_score(y_test, prediction5)\n",
        "f1 = f1_score(y_test, prediction5)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4601e7d3",
      "metadata": {
        "id": "4601e7d3"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,prediction5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534adff0",
      "metadata": {
        "id": "534adff0"
      },
      "outputs": [],
      "source": [
        "probs = Model1.predict_proba(x_test)\n",
        "precision_score(y_test, prediction5, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef970ecc",
      "metadata": {
        "id": "ef970ecc"
      },
      "outputs": [],
      "source": [
        "recall_score(y_test, prediction5, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae742763",
      "metadata": {
        "id": "ae742763"
      },
      "outputs": [],
      "source": [
        "f1_score(y_test, prediction5, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6324e83d",
      "metadata": {
        "id": "6324e83d"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true = y_test, y_pred = prediction5)\n",
        "#plot_confusion_matrix(cm,level,title = \"confusion_matrix\")\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6e18fd",
      "metadata": {
        "id": "ae6e18fd"
      },
      "source": [
        "RANDOM FOREST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33d6921",
      "metadata": {
        "id": "c33d6921"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model using training dataset\n",
        "rf_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on test dataset\n",
        "prediction6 = rf_classifier.predict(x_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "#accuracy = rf_classifier.score(x_test, y_test)\n",
        "#print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20710a32",
      "metadata": {
        "id": "20710a32"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test,prediction6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a1091e",
      "metadata": {
        "id": "b4a1091e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# assuming your predicted and actual labels are stored in variables y_pred and y_true, respectively\n",
        "accuracy = accuracy_score(y_test, prediction6)\n",
        "precision = precision_score(y_test, prediction6)\n",
        "recall = recall_score(y_test, prediction6)\n",
        "f1 = f1_score(y_test, prediction6)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ee17a4",
      "metadata": {
        "scrolled": true,
        "id": "a6ee17a4"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test,prediction6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a009bbc9",
      "metadata": {
        "id": "a009bbc9"
      },
      "outputs": [],
      "source": [
        "probs = Model1.predict_proba(x_test)\n",
        "precision_score(y_test, prediction6, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a221dbf",
      "metadata": {
        "id": "8a221dbf"
      },
      "outputs": [],
      "source": [
        "recall_score(y_test, prediction6, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e39486",
      "metadata": {
        "id": "91e39486"
      },
      "outputs": [],
      "source": [
        "f1_score(y_test, prediction6, average = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec409b0a",
      "metadata": {
        "id": "ec409b0a"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true = y_test, y_pred = prediction6)\n",
        "#plot_confusion_matrix(cm,level,title = \"confusion_matrix\")\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62bfe22b",
      "metadata": {
        "id": "62bfe22b"
      },
      "source": [
        "========================================================================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69577070",
      "metadata": {
        "id": "69577070"
      },
      "outputs": [],
      "source": [
        "#Finding Correlation\n",
        "cn=lung_data.corr()\n",
        "cn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5293e5",
      "metadata": {
        "id": "8a5293e5"
      },
      "outputs": [],
      "source": [
        "#Correlation\n",
        "cmap=sns.diverging_palette(260,-10,s=50, l=75, n=6,\n",
        "as_cmap=True)\n",
        "plt.subplots(figsize=(18,18))\n",
        "sns.heatmap(cn,cmap=\"Blues\",annot=True, square=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766a0bca",
      "metadata": {
        "id": "766a0bca"
      },
      "outputs": [],
      "source": [
        "num_list = list(lung_data.columns)\n",
        "\n",
        "fig = plt.figure(figsize=(10,30))\n",
        "\n",
        "for i in range(len(num_list)):\n",
        "    plt.subplot(8,2,i+1)\n",
        "    plt.title(num_list[i])\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.hist(lung_data[num_list[i]],color='blue',alpha=0.5)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d802aa",
      "metadata": {
        "id": "66d802aa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}